{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Exploring the Mountain Car Environment\n",
        "\n",
        "In this part we set up and explore the Mountain Car environment and inspect how a simple fixed policy behaves.\n",
        "\n",
        "There are **three code cells**:\n",
        "\n",
        "1. **Setup** — creates the environment, defines helper functions, and sets up a simple heuristic policy.\n",
        "2. **Plots** — runs the policy once and plots position, velocity, and the phase plot.\n",
        "3. **Video** — records a short animation showing how the policy behaves.\n",
        "\n",
        "Run all cells and make sure you understand what the plots show.\n"
      ],
      "metadata": {
        "id": "6Rg59GYOYrIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkuQJvvc6HwX"
      },
      "outputs": [],
      "source": [
        "# === Setup and helper functions ===\n",
        "\n",
        "# Install and import\n",
        "!pip install gymnasium[classic_control] --quiet\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "\n",
        "# Create environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"  low :\", env.observation_space.low)\n",
        "print(\"  high:\", env.observation_space.high)\n",
        "print()\n",
        "print(\"Action space:\", env.action_space)\n",
        "print(\"Number of actions:\", env.action_space.n)\n",
        "\n",
        "# ----- Helper: run one episode with a given policy -----\n",
        "def run_episode(env, policy, max_steps=200):\n",
        "    \"\"\"\n",
        "    Run one episode using the given policy.\n",
        "    policy: function that maps state -> action (integer).\n",
        "    Returns: states, actions, rewards as numpy arrays.\n",
        "    \"\"\"\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        states.append(state)\n",
        "        action = policy(state)\n",
        "        actions.append(action)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards)\n",
        "\n",
        "# ----- Policy: \"greedy towards the goal\" (but actually bad) -----\n",
        "def greedy_towards_goal_policy(state):\n",
        "    position, velocity = state\n",
        "    goal_position = 0.5  # flag on the right hill\n",
        "\n",
        "    # Naïve idea: if goal is to the right, always push right\n",
        "    if position < goal_position:\n",
        "        return 2  # push right\n",
        "    else:\n",
        "        return 0  # almost never reached in practice\n",
        "\n",
        "# ----- Helper: plot a trajectory -----\n",
        "def plot_trajectory(states, title_prefix=\"\"):\n",
        "    pos = states[:, 0]\n",
        "    vel = states[:, 1]\n",
        "    t = np.arange(len(pos))\n",
        "\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
        "\n",
        "    axs[0].plot(t, pos)\n",
        "    axs[0].axhline(0.5, linestyle=\"--\", color=\"tab:red\")\n",
        "    axs[0].set_ylabel(\"Position\")\n",
        "    axs[0].set_title(f\"{title_prefix}Position over time\")\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    axs[1].plot(t, vel)\n",
        "    axs[1].set_ylabel(\"Velocity\")\n",
        "    axs[1].set_title(f\"{title_prefix}Velocity over time\")\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    axs[2].plot(pos, vel, marker=\".\")\n",
        "    axs[2].set_xlabel(\"Position\")\n",
        "    axs[2].set_ylabel(\"Velocity\")\n",
        "    axs[2].set_title(f\"{title_prefix}Phase plot (position vs velocity)\")\n",
        "    axs[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Run the naïve policy and inspect behaviour ===\n",
        "\n",
        "states_greedy, actions_greedy, rewards_greedy = run_episode(env, greedy_towards_goal_policy)\n",
        "\n",
        "print(\"Greedy-towards-goal policy:\")\n",
        "print(\"  Episode length:\", len(rewards_greedy))\n",
        "print(\"  Total return (sum of rewards):\", rewards_greedy.sum())\n",
        "\n",
        "plot_trajectory(states_greedy, title_prefix=\"Greedy-to-goal policy: \")\n"
      ],
      "metadata": {
        "id": "9vjXFbCB63nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === create and display a video of the failing policy ===\n",
        "\n",
        "!pip install imageio --quiet\n",
        "import imageio\n",
        "from IPython.display import Video\n",
        "\n",
        "env_video = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "def record_policy(env, policy, filename=\"mountaincar_greedy_fail.mp4\", max_steps=200):\n",
        "    frames = []\n",
        "    state, info = env.reset()\n",
        "    for t in range(max_steps):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        action = policy(state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "    return filename\n",
        "\n",
        "video_path = record_policy(env_video, greedy_towards_goal_policy)\n",
        "Video(video_path, embed=True)\n"
      ],
      "metadata": {
        "id": "bFqeJXkQ76p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Linear and Polynomial Function Approximation\n",
        "\n",
        "In this part we evaluate a fixed heuristic policy using simple feature representations.\n",
        "\n",
        "There are **three code cells**:\n",
        "\n",
        "1. **Policy** — defines the heuristic policy we want to evaluate.\n",
        "\n",
        "2. **Reference Value** — computes a high-accuracy value function on a grid using brute-force Monte Carlo simulation.  \n",
        "   *This is only possible here because Mountain Car has a small, two-dimensional state space **and** the simulator allows us to force the system into any state we like. In most RL problems neither of these is true: the state space is too large to grid-sample, and we cannot arbitrarily reset the environment to every possible state. For real problems, learning methods (MC/TD) are the only feasible option.*\n",
        "\n",
        "3. **MC and TD(0)** — runs Monte Carlo and TD learning with linear and polynomial features and plots the resulting approximations.\n",
        "\n",
        "Run all cells and compare the approximations to the reference value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K_lcjAphY_xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Policy to evaluate (good heuristic policy) -----\n",
        "def good_policy(state):\n",
        "  # Push in the direction of current velocity\n",
        "    x, v = state\n",
        "    return 2 if v >= 0 else 0   # push in direction of motion\n"
      ],
      "metadata": {
        "id": "c4zBGzxTZA-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 2: Linear Value Function Approximation with Reference  ===\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "\n",
        "# ----- Environment -----\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "POS_MIN, POS_MAX = env.observation_space.low[0], env.observation_space.high[0]\n",
        "VEL_MIN, VEL_MAX = env.observation_space.low[1], env.observation_space.high[1]\n",
        "\n",
        "gamma = 1.0\n",
        "\n",
        "\n",
        "# ----- State normalization and features -----\n",
        "def normalize_state(state):\n",
        "    x, v = state\n",
        "    x_n = (x - POS_MIN) / (POS_MAX - POS_MIN) * 2 - 1\n",
        "    v_n = (v - VEL_MIN) / (VEL_MAX - VEL_MIN) * 2 - 1\n",
        "    return x_n, v_n\n",
        "\n",
        "def phi_raw(s):\n",
        "    x_n, v_n = normalize_state(s)\n",
        "    return np.array([1.0, x_n, v_n], dtype=float)\n",
        "\n",
        "def phi_poly(s):\n",
        "    x_n, v_n = normalize_state(s)\n",
        "    return np.array([\n",
        "        1.0,\n",
        "        x_n,\n",
        "        v_n,\n",
        "        x_n**2,\n",
        "        v_n**2,\n",
        "        x_n * v_n\n",
        "    ], dtype=float)\n",
        "\n",
        "# ----- Run one episode from current env.state -----\n",
        "def run_episode_from_current(env, policy):\n",
        "    \"\"\"Run an episode starting from env.unwrapped.state.\"\"\"\n",
        "    s = np.array(env.unwrapped.state, dtype=float)\n",
        "    G = 0.0\n",
        "    done = False\n",
        "    while not done:\n",
        "        a = policy(s)\n",
        "        s, r, terminated, truncated, info = env.step(a)\n",
        "        G += r\n",
        "        done = terminated or truncated\n",
        "    return G\n",
        "\n",
        "# ----- Brute-force reference value on a grid -----\n",
        "def estimate_reference(env, policy, n_x=31, n_v=31, N_MC=20):\n",
        "    xs = np.linspace(POS_MIN, POS_MAX, n_x)\n",
        "    vs = np.linspace(VEL_MIN, VEL_MAX, n_v)\n",
        "    V = np.zeros((n_x, n_v))\n",
        "\n",
        "    for i, x in enumerate(xs):\n",
        "        for j, v in enumerate(vs):\n",
        "            returns = []\n",
        "            for _ in range(N_MC):\n",
        "                env.reset()\n",
        "                env.unwrapped.state = np.array([x, v], dtype=float)\n",
        "                G = run_episode_from_current(env, policy)\n",
        "                returns.append(G)\n",
        "            V[i, j] = np.mean(returns)\n",
        "    return xs, vs, V\n",
        "\n",
        "print(\"Computing reference value function for the good policy (this may take ~20–30 seconds)...\")\n",
        "xs_ref, vs_ref, V_ref = estimate_reference(env, good_policy, n_x=31, n_v=31, N_MC=20)\n",
        "print(\"Reference value function computed.\")\n"
      ],
      "metadata": {
        "id": "PmDtZ584gsqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 2: MC and TD(0) with RAW and POLYNOMIAL features ===\n",
        "\n",
        "# ----- Helper: run one episode with standard reset -----\n",
        "def run_episode(env, policy, max_steps=200):\n",
        "    states, rewards = [], []\n",
        "    s, info = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < max_steps:\n",
        "        states.append(s)\n",
        "        a = policy(s)\n",
        "        s, r, terminated, truncated, info = env.step(a)\n",
        "        rewards.append(r)\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "    return np.array(states), np.array(rewards)\n",
        "\n",
        "# ----- Monte Carlo prediction -----\n",
        "def mc_prediction(env, policy, phi_fn, alpha=1e-3, gamma=1.0, num_episodes=3000):\n",
        "    d = phi_fn(env.reset()[0]).shape[0]\n",
        "    w = np.zeros(d, dtype=float)\n",
        "    for _ in range(num_episodes):\n",
        "        states, rewards = run_episode(env, policy)\n",
        "        G = 0.0\n",
        "        for s, r in zip(states[::-1], rewards[::-1]):\n",
        "            G = r + gamma * G\n",
        "            phi = phi_fn(s)\n",
        "            v_hat = np.dot(w, phi)\n",
        "            w += alpha * (G - v_hat) * phi\n",
        "    return w\n",
        "\n",
        "# ----- TD(0) prediction -----\n",
        "def td0_prediction(env, policy, phi_fn, alpha=1e-3, gamma=1.0, num_episodes=3000):\n",
        "    d = phi_fn(env.reset()[0]).shape[0]\n",
        "    w = np.zeros(d, dtype=float)\n",
        "    for _ in range(num_episodes):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            phi = phi_fn(s)\n",
        "            a = policy(s)\n",
        "            s2, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                target = r\n",
        "            else:\n",
        "                target = r + gamma * np.dot(w, phi_fn(s2))\n",
        "\n",
        "            v_hat = np.dot(w, phi)\n",
        "            w += alpha * (target - v_hat) * phi\n",
        "            s = s2\n",
        "    return w\n",
        "\n",
        "# ----- RMSE on reference grid -----\n",
        "def compute_rmse(w, phi_fn, xs, vs, V_ref):\n",
        "    sq_errs = []\n",
        "    for i, x in enumerate(xs):\n",
        "        for j, v in enumerate(vs):\n",
        "            s = np.array([x, v], dtype=float)\n",
        "            v_hat = np.dot(w, phi_fn(s))\n",
        "            sq_errs.append((v_hat - V_ref[i, j])**2)\n",
        "    return np.sqrt(np.mean(sq_errs))\n",
        "\n",
        "# ----- Train MC/TD for raw and polynomial features -----\n",
        "print(\"Training MC/TD with RAW features...\")\n",
        "w_mc_raw = mc_prediction(env, good_policy, phi_raw, alpha=1e-3, gamma=gamma, num_episodes=8000)\n",
        "w_td_raw = td0_prediction(env, good_policy, phi_raw, alpha=1e-3, gamma=gamma, num_episodes=8000)\n",
        "\n",
        "print(\"Training MC/TD with POLYNOMIAL features...\")\n",
        "w_mc_poly = mc_prediction(env, good_policy, phi_poly, alpha=5e-4, gamma=gamma, num_episodes=8000)\n",
        "w_td_poly = td0_prediction(env, good_policy, phi_poly, alpha=5e-4, gamma=gamma, num_episodes=8000)\n",
        "\n",
        "rmse_mc_raw  = compute_rmse(w_mc_raw,  phi_raw,  xs_ref, vs_ref, V_ref)\n",
        "rmse_td_raw  = compute_rmse(w_td_raw,  phi_raw,  xs_ref, vs_ref, V_ref)\n",
        "rmse_mc_poly = compute_rmse(w_mc_poly, phi_poly, xs_ref, vs_ref, V_ref)\n",
        "rmse_td_poly = compute_rmse(w_td_poly, phi_poly, xs_ref, vs_ref, V_ref)\n",
        "\n",
        "print(\"\\n=== RMSE vs reference (good policy) ===\")\n",
        "print(f\"MC  + RAW features:        {rmse_mc_raw:.2f}\")\n",
        "print(f\"TD(0) + RAW features:      {rmse_td_raw:.2f}\")\n",
        "print(f\"MC  + POLY features:       {rmse_mc_poly:.2f}\")\n",
        "print(f\"TD(0) + POLY features:     {rmse_td_poly:.2f}\")\n",
        "\n",
        "# ----- Plot along position axis at v = 0 -----\n",
        "def plot_vs_position(w_list, labels, phi_fns, xs_ref, vs_ref, V_ref, title=\"\"):\n",
        "    xs_plot = np.linspace(POS_MIN, POS_MAX, 200)\n",
        "    v0 = 0.0\n",
        "\n",
        "    # Reference slice at v ~ 0\n",
        "    j_mid = len(vs_ref) // 2\n",
        "    Vref_slice = np.interp(xs_plot, xs_ref, V_ref[:, j_mid])\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(xs_plot, Vref_slice, label=\"Reference\", linewidth=3, color=\"black\")\n",
        "\n",
        "    for w, label, phi_fn in zip(w_list, labels, phi_fns):\n",
        "        vals = [np.dot(w, phi_fn([x, v0])) for x in xs_plot]\n",
        "        plt.plot(xs_plot, vals, label=label)\n",
        "\n",
        "    plt.axvline(0.5, color=\"red\", linestyle=\"--\", label=\"Goal position\")\n",
        "    plt.xlabel(\"Position x (velocity = 0)\")\n",
        "    plt.ylabel(\"Estimated value\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_vs_position(\n",
        "    w_list=[w_mc_raw, w_td_raw, w_mc_poly, w_td_poly],\n",
        "    labels=[\"MC RAW\", \"TD(0) RAW\", \"MC POLY\", \"TD(0) POLY\"],\n",
        "    phi_fns=[phi_raw, phi_raw, phi_poly, phi_poly],\n",
        "    xs_ref=xs_ref,\n",
        "    vs_ref=vs_ref,\n",
        "    V_ref=V_ref,\n",
        "    title=\"Value function along position (good policy)\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "YVQq_Cxll5jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Tile Coding and State Visitation\n",
        "\n",
        "In this part we use tile coding to obtain a richer approximation of the value function.\n",
        "\n",
        "There are **three code cells**:\n",
        "\n",
        "1. **MC and TD + Tile Coding** — implements tile coding and runs Monte Carlo and TD learning with the tile-coded features.\n",
        "2. **Plots** — compares the tile-coded approximation to the reference value along the position axis for different velocities.\n",
        "3. **Visitation Heatmap** — visualizes which parts of the state space the policy actually visits.\n",
        "\n",
        "Run all cells and relate approximation quality to the visitation heatmap.\n"
      ],
      "metadata": {
        "id": "yLqcpuqkn4hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 3: Tile Coding + MC and TD(0) ===\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# TILE CODING PARAMETERS (ADJUST HERE)\n",
        "# -------------------------------------------------------------------\n",
        "# Trade-offs:\n",
        "# - More tilings / more tiles per dimension -> better approximation, but:\n",
        "#     * more features, more memory\n",
        "#     * slower updates (more computation per step)\n",
        "#     * usually needs more episodes to converge well.\n",
        "# - Fewer tilings / fewer tiles -> faster but coarser approximation.\n",
        "\n",
        "N_TILINGS = 16      # e.g. 8 or 16\n",
        "TILES_POS = 16     # number of tiles along position axis (e.g. 8–16)\n",
        "TILES_VEL = 16     # number of tiles along velocity axis (e.g. 8–16)\n",
        "\n",
        "\n",
        "TILES_PER_TILING = TILES_POS * TILES_VEL\n",
        "FEATURE_DIM = N_TILINGS * TILES_PER_TILING\n",
        "\n",
        "# Small offsets so tilings are shifted\n",
        "offsets = np.linspace(0.0, 1.0, N_TILINGS, endpoint=False)\n",
        "\n",
        "def normalize01(state):\n",
        "    \"\"\"Normalize position and velocity to [0,1].\"\"\"\n",
        "    x, v = state\n",
        "    x01 = (x - POS_MIN) / (POS_MAX - POS_MIN)\n",
        "    v01 = (v - VEL_MIN) / (VEL_MAX - VEL_MIN)\n",
        "    # clamp just in case of numerical issues\n",
        "    x01 = min(max(x01, 0.0), 0.999999)\n",
        "    v01 = min(max(v01, 0.0), 0.999999)\n",
        "    return x01, v01\n",
        "\n",
        "def phi_tile(state):\n",
        "    \"\"\"\n",
        "    Tile-coded features for state value:\n",
        "    - N_TILINGS overlapping grids.\n",
        "    - One active tile per tiling -> N_TILINGS active features.\n",
        "    Returns a dense 0/1 feature vector of length FEATURE_DIM.\n",
        "    \"\"\"\n",
        "    x01, v01 = normalize01(state)\n",
        "    features = np.zeros(FEATURE_DIM, dtype=float)\n",
        "\n",
        "    for tiling in range(N_TILINGS):\n",
        "        # Shift and wrap into [0,1)\n",
        "        x_shifted = (x01 + offsets[tiling] / TILES_POS) % 1.0\n",
        "        v_shifted = (v01 + offsets[tiling] / TILES_VEL) % 1.0\n",
        "\n",
        "        i_pos = int(x_shifted * TILES_POS)\n",
        "        i_vel = int(v_shifted * TILES_VEL)\n",
        "\n",
        "        # Safety clamp\n",
        "        i_pos = min(i_pos, TILES_POS - 1)\n",
        "        i_vel = min(i_vel, TILES_VEL - 1)\n",
        "\n",
        "        tile_index = i_pos * TILES_VEL + i_vel\n",
        "        feat_index = tiling * TILES_PER_TILING + tile_index\n",
        "        features[feat_index] = 1.0\n",
        "\n",
        "    return features\n",
        "\n",
        "# run one episode (same structure as before) -----\n",
        "def run_episode(env, policy, max_steps=200):\n",
        "    states, rewards = [], []\n",
        "    s, info = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < max_steps:\n",
        "        states.append(s)\n",
        "        a = policy(s)\n",
        "        s, r, terminated, truncated, info = env.step(a)\n",
        "        rewards.append(r)\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "    return np.array(states), np.array(rewards)\n",
        "\n",
        "# ----- Linear value with generic features -----\n",
        "def v_hat(state, w, phi_fn):\n",
        "    return np.dot(w, phi_fn(state))\n",
        "\n",
        "# ----- Monte Carlo prediction (generic features) -----\n",
        "def mc_prediction(env, policy, phi_fn, alpha=1e-2, gamma=1.0, num_episodes=5000):\n",
        "    d = phi_fn(env.reset()[0]).shape[0]\n",
        "    w = np.zeros(d, dtype=float)\n",
        "    for _ in range(num_episodes):\n",
        "        states, rewards = run_episode(env, policy)\n",
        "        G = 0.0\n",
        "        for s, r in zip(states[::-1], rewards[::-1]):\n",
        "            G = r + gamma * G\n",
        "            phi = phi_fn(s)\n",
        "            v = np.dot(w, phi)\n",
        "            w += alpha * (G - v) * phi\n",
        "    return w\n",
        "\n",
        "# ----- TD(0) prediction (generic features) -----\n",
        "def td0_prediction(env, policy, phi_fn, alpha=1e-2, gamma=1.0, num_episodes=5000):\n",
        "    d = phi_fn(env.reset()[0]).shape[0]\n",
        "    w = np.zeros(d, dtype=float)\n",
        "    for _ in range(num_episodes):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            phi = phi_fn(s)\n",
        "            a = policy(s)\n",
        "            s2, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                target = r\n",
        "            else:\n",
        "                target = r + gamma * np.dot(w, phi_fn(s2))\n",
        "\n",
        "            v = np.dot(w, phi)\n",
        "            w += alpha * (target - v) * phi\n",
        "            s = s2\n",
        "    return w\n",
        "\n",
        "# ----- RMSE on reference grid -----\n",
        "def compute_rmse(w, phi_fn, xs, vs, V_ref):\n",
        "    sq_errs = []\n",
        "    for i, x in enumerate(xs):\n",
        "        for j, v in enumerate(vs):\n",
        "            s = np.array([x, v], dtype=float)\n",
        "            v_hat_val = np.dot(w, phi_fn(s))\n",
        "            sq_errs.append((v_hat_val - V_ref[i, j])**2)\n",
        "    return np.sqrt(np.mean(sq_errs))\n",
        "\n",
        "# ===== Train MC/TD with tile coding =====\n",
        "alpha_tile = 0.2 / N_TILINGS\n",
        "num_episodes = 10000\n",
        "\n",
        "print(\"Training MC + tile coding...\")\n",
        "w_mc_tile = mc_prediction(env, good_policy, phi_tile,\n",
        "                          alpha=alpha_tile, gamma=gamma, num_episodes=num_episodes)\n",
        "\n",
        "print(\"Training TD(0) + tile coding...\")\n",
        "w_td_tile = td0_prediction(env, good_policy, phi_tile,\n",
        "                           alpha=alpha_tile, gamma=gamma, num_episodes=num_episodes)\n",
        "\n",
        "rmse_mc_tile = compute_rmse(w_mc_tile, phi_tile, xs_ref, vs_ref, V_ref)\n",
        "rmse_td_tile = compute_rmse(w_td_tile, phi_tile, xs_ref, vs_ref, V_ref)\n",
        "\n",
        "print(\"\\n=== RMSE vs reference (good policy, tile coding) ===\")\n",
        "print(f\"MC  + tile coding:  {rmse_mc_tile:.2f}\")\n",
        "print(f\"TD(0) + tile coding:{rmse_td_tile:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0NDGKa9woCv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Plot the value function along the position axis for a chosen velocity v0 -----\n",
        "def plot_vs_position_tile(w_mc, w_td, phi_fn, xs_ref, vs_ref, V_ref,\n",
        "                          v0=0.0, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots the value function along the position axis for a fixed velocity v0.\n",
        "\n",
        "    Parameters:\n",
        "    - v0: velocity at which to slice the value function. You should experiment\n",
        "          with values in the range [-0.07, 0.07], which covers all velocities in Mountain Car.\n",
        "    \"\"\"\n",
        "\n",
        "    # Range of x values to plot\n",
        "    xs_plot = np.linspace(POS_MIN, POS_MAX, 200)\n",
        "\n",
        "    # Find the closest velocity index in the reference grid for v0\n",
        "    j = np.argmin(np.abs(vs_ref - v0))\n",
        "\n",
        "    # Get the reference value slice at velocity closest to v0\n",
        "    Vref_slice = np.interp(xs_plot, xs_ref, V_ref[:, j])\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Plot reference slice\n",
        "    plt.plot(xs_plot, Vref_slice,\n",
        "             label=f\"Reference (v ≈ {vs_ref[j]:.3f})\",\n",
        "             linewidth=3, color=\"black\")\n",
        "\n",
        "    # MC and TD(0) approximations at v0\n",
        "    vals_mc = [np.dot(w_mc, phi_fn([x, v0])) for x in xs_plot]\n",
        "    vals_td = [np.dot(w_td, phi_fn([x, v0])) for x in xs_plot]\n",
        "\n",
        "    plt.plot(xs_plot, vals_mc, label=f\"MC approx (v={v0:.3f})\")\n",
        "    plt.plot(xs_plot, vals_td, label=f\"TD(0) approx (v={v0:.3f})\")\n",
        "\n",
        "    plt.axvline(0.5, color=\"red\", linestyle=\"--\", label=\"Goal position\")\n",
        "\n",
        "    plt.xlabel(\"Position x\")\n",
        "    plt.ylabel(\"Estimated value\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------ Change v0 here ------\n",
        "# Try different values in the range [-0.07, 0.07], e.g.:\n",
        "# v0 = -0.03\n",
        "# v0 = 0.0\n",
        "# v0 = 0.02\n",
        "v0 = 0.0   # <-- CHANGE THIS VALUE TO EXPLORE DIFFERENT VELOCITY SLICES\n",
        "\n",
        "\n",
        "# Plot the chosen slice\n",
        "plot_vs_position_tile(\n",
        "    w_mc_tile, w_td_tile, phi_tile,\n",
        "    xs_ref, vs_ref, V_ref, v0,\n",
        "    title=\"Value function along position (tile coding, good policy)\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "C9gnzSa4pEfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Extra diagnostics: value slices and visitation heatmap ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visitation heatmap: where does the policy actually go in (x, v)?\n",
        "def collect_visits(env, policy, num_episodes=500, max_steps=200):\n",
        "    xs, vs = [], []\n",
        "    for _ in range(num_episodes):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps < max_steps:\n",
        "            xs.append(s[0])\n",
        "            vs.append(s[1])\n",
        "            a = policy(s)\n",
        "            s, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            steps += 1\n",
        "    return np.array(xs), np.array(vs)\n",
        "\n",
        "xs_vis, vs_vis = collect_visits(env, good_policy, num_episodes=500)\n",
        "\n",
        "# Bin visits into a grid for heatmap\n",
        "num_bins_x = 60\n",
        "num_bins_v = 60\n",
        "H, xedges, vedges = np.histogram2d(xs_vis, vs_vis,\n",
        "                                   bins=[num_bins_x, num_bins_v],\n",
        "                                   range=[[POS_MIN, POS_MAX],\n",
        "                                          [VEL_MIN, VEL_MAX]])\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "extent = [xedges[0], xedges[-1], vedges[0], vedges[-1]]\n",
        "plt.imshow(H.T, origin=\"lower\", extent=extent, aspect=\"auto\")\n",
        "plt.colorbar(label=\"Visit count\")\n",
        "plt.xlabel(\"Position x\")\n",
        "plt.ylabel(\"Velocity v\")\n",
        "plt.title(\"State visitation heatmap under good_policy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "naZi6smKoIzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Control with Sarsa and Tile Coding\n",
        "\n",
        "In this part we switch from policy evaluation to **control**, using Sarsa(0) with\n",
        "tile-coded action–value features to learn a policy that solves Mountain Car.\n",
        "\n",
        "There are **two code cells**:\n",
        "\n",
        "1. **Sarsa(0) Training + Learning Curve** — trains the agent with tile coding and plots the\n",
        "   number of steps per episode (and a moving average) to show learning progress.\n",
        "\n",
        "2. **Video of Learned Policy** — runs the greedy policy after training and records a short\n",
        "   video so you can observe how the agent improves over time.\n",
        "\n",
        "Run both cells and compare the learned behaviour to the policies from earlier parts.\n"
      ],
      "metadata": {
        "id": "Tyy6sFvdGNdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 4: Sarsa(0) control with tile coding (self-contained) ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ENVIRONMENT SETUP\n",
        "# -------------------------------------------------------------------\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "n_actions = env.action_space.n  # 3 actions\n",
        "\n",
        "POS_MIN, POS_MAX = env.observation_space.low[0], env.observation_space.high[0]\n",
        "VEL_MIN, VEL_MAX = env.observation_space.low[1], env.observation_space.high[1]\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# TILE CODING PARAMETERS (ADJUST HERE)\n",
        "# -------------------------------------------------------------------\n",
        "# Trade-offs:\n",
        "# - More tilings / more tiles per dimension -> better approximation, but:\n",
        "#     * more features, more memory\n",
        "#     * slower updates (more computation per step)\n",
        "#     * usually needs more episodes to converge well.\n",
        "# - Fewer tilings / fewer tiles -> faster but coarser approximation.\n",
        "\n",
        "N_TILINGS = 8      # e.g. 8 or 16\n",
        "TILES_POS = 12     # number of tiles along position axis (e.g. 8–16)\n",
        "TILES_VEL = 12     # number of tiles along velocity axis (e.g. 8–16)\n",
        "\n",
        "TILES_PER_TILING = TILES_POS * TILES_VEL\n",
        "FEATURE_DIM = N_TILINGS * TILES_PER_TILING\n",
        "\n",
        "# Small offsets to shift each tiling (for better generalization)\n",
        "offsets = np.linspace(0.0, 1.0, N_TILINGS, endpoint=False)\n",
        "\n",
        "def normalize01(state):\n",
        "    \"\"\"Normalize (x, v) to the range [0, 1] x [0, 1].\"\"\"\n",
        "    x, v = state\n",
        "    x01 = (x - POS_MIN) / (POS_MAX - POS_MIN)\n",
        "    v01 = (v - VEL_MIN) / (VEL_MAX - VEL_MIN)\n",
        "    # clamp for numerical safety\n",
        "    x01 = min(max(x01, 0.0), 0.999999)\n",
        "    v01 = min(max(v01, 0.0), 0.999999)\n",
        "    return x01, v01\n",
        "\n",
        "def phi_tile(state):\n",
        "    \"\"\"\n",
        "    Tile-coded features for a state:\n",
        "    - N_TILINGS overlapping grids\n",
        "    - One active tile per tiling -> N_TILINGS active features in total.\n",
        "    \"\"\"\n",
        "    x01, v01 = normalize01(state)\n",
        "    features = np.zeros(FEATURE_DIM, dtype=float)\n",
        "\n",
        "    for tiling in range(N_TILINGS):\n",
        "        # Shift and wrap into [0, 1)\n",
        "        x_shifted = (x01 + offsets[tiling] / TILES_POS) % 1.0\n",
        "        v_shifted = (v01 + offsets[tiling] / TILES_VEL) % 1.0\n",
        "\n",
        "        i_pos = int(x_shifted * TILES_POS)\n",
        "        i_vel = int(v_shifted * TILES_VEL)\n",
        "\n",
        "        # Safety clamp\n",
        "        i_pos = min(i_pos, TILES_POS - 1)\n",
        "        i_vel = min(i_vel, TILES_VEL - 1)\n",
        "\n",
        "        tile_index = i_pos * TILES_VEL + i_vel\n",
        "        feat_index = tiling * TILES_PER_TILING + tile_index\n",
        "        features[feat_index] = 1.0\n",
        "\n",
        "    return features\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# SARSA / LEARNING PARAMETERS (ADJUST HERE)\n",
        "# -------------------------------------------------------------------\n",
        "# Number of training episodes:\n",
        "#   fewer  -> faster but typically worse policy,\n",
        "#   more   -> slower but typically better policy.\n",
        "SARSA_NUM_EPISODES = 8000\n",
        "\n",
        "# Step size:\n",
        "#   alpha ≈ ALPHA_PER_TILING / N_TILINGS is a common heuristic.\n",
        "#   larger -> faster learning but more unstable,\n",
        "#   smaller -> slower but more stable.\n",
        "ALPHA_PER_TILING = 0.3           # effective alpha = ALPHA_PER_TILING / N_TILINGS\n",
        "\n",
        "# Discount factor (keep at 1.0 for Mountain Car)\n",
        "SARSA_GAMMA = 1.0\n",
        "\n",
        "# Exploration schedule (epsilon-greedy):\n",
        "#   EPS_START: initial exploration rate (higher -> more exploration early).\n",
        "#   EPS_END:   minimum exploration rate.\n",
        "#   EPS_DECAY_RATE: how fast epsilon decays (larger -> slower decay).\n",
        "EPS_START = 0.2\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY_RATE = 4000\n",
        "\n",
        "# Moving-average window for the learning curve:\n",
        "#   larger -> smoother curve, but slower to reflect changes.\n",
        "MOVING_AVG_WINDOW = 100\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# ----- Action-value features: tile coding for (state, action) -----\n",
        "def phi_tile_sa(state, action):\n",
        "    \"\"\"\n",
        "    Tile-coded features for a state-action pair.\n",
        "    We allocate one block of FEATURES per action, and use the tile-coded\n",
        "    state features in the block corresponding to the given action.\n",
        "    \"\"\"\n",
        "    z = phi_tile(state)  # length FEATURE_DIM\n",
        "    features = np.zeros(FEATURE_DIM * n_actions, dtype=float)\n",
        "    start = action * FEATURE_DIM\n",
        "    features[start:start + FEATURE_DIM] = z\n",
        "    return features\n",
        "\n",
        "def q_hat(state, action, w):\n",
        "    return np.dot(w, phi_tile_sa(state, action))\n",
        "\n",
        "# ----- Epsilon-greedy policy -----\n",
        "def epsilon_greedy_action(state, w, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    qs = [q_hat(state, a, w) for a in range(n_actions)]\n",
        "    return int(np.argmax(qs))\n",
        "\n",
        "# ----- Sarsa(0) control with tile coding -----\n",
        "def sarsa_control(env, num_episodes, alpha, gamma,\n",
        "                  eps_start, eps_end, decay_rate):\n",
        "    \"\"\"\n",
        "    Semi-gradient Sarsa(0) with tile-coded action-value features.\n",
        "\n",
        "    Parameters:\n",
        "    - num_episodes: number of training episodes.\n",
        "    - alpha: step size.\n",
        "    - gamma: discount factor.\n",
        "    - eps_start, eps_end, decay_rate: parameters for epsilon decay.\n",
        "\n",
        "    Returns:\n",
        "    - w: learned weight vector\n",
        "    - returns: episode returns (sum of rewards)\n",
        "    - steps: number of steps in each episode\n",
        "    \"\"\"\n",
        "    d = FEATURE_DIM * n_actions\n",
        "    w = np.zeros(d, dtype=float)\n",
        "\n",
        "    returns = []\n",
        "    steps_list = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        # Exponential epsilon decay\n",
        "        epsilon = eps_end + (eps_start - eps_end) * np.exp(-ep / decay_rate)\n",
        "\n",
        "        state, info = env.reset()\n",
        "        action = epsilon_greedy_action(state, w, epsilon)\n",
        "\n",
        "        done = False\n",
        "        G = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            G += reward\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                target = reward\n",
        "                delta = target - q_hat(state, action, w)\n",
        "                w += alpha * delta * phi_tile_sa(state, action)\n",
        "            else:\n",
        "                next_action = epsilon_greedy_action(next_state, w, epsilon)\n",
        "                target = reward + gamma * q_hat(next_state, next_action, w)\n",
        "                delta = target - q_hat(state, action, w)\n",
        "                w += alpha * delta * phi_tile_sa(state, action)\n",
        "                state, action = next_state, next_action\n",
        "\n",
        "        returns.append(G)\n",
        "        steps_list.append(steps)\n",
        "\n",
        "        if (ep + 1) % 500 == 0:\n",
        "            print(f\"Episode {ep+1}/{num_episodes}, steps = {steps}, return = {G:.1f}\")\n",
        "\n",
        "    return w, np.array(returns), np.array(steps_list)\n",
        "\n",
        "# ----- Run Sarsa(0) and plot learning curve -----\n",
        "alpha_sarsa = ALPHA_PER_TILING / N_TILINGS  # actual alpha used\n",
        "\n",
        "w_sarsa, ep_returns, ep_steps = sarsa_control(\n",
        "    env,\n",
        "    num_episodes=SARSA_NUM_EPISODES,\n",
        "    alpha=alpha_sarsa,\n",
        "    gamma=SARSA_GAMMA,\n",
        "    eps_start=EPS_START,\n",
        "    eps_end=EPS_END,\n",
        "    decay_rate=EPS_DECAY_RATE\n",
        ")\n",
        "\n",
        "# Plot steps per episode and moving average\n",
        "window = MOVING_AVG_WINDOW\n",
        "ma_steps = np.convolve(ep_steps, np.ones(window) / window, mode=\"valid\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ep_steps, alpha=0.3, label=\"Steps per episode\")\n",
        "plt.plot(np.arange(window - 1, SARSA_NUM_EPISODES), ma_steps,\n",
        "         label=f\"{window}-episode moving average\", linewidth=2)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Steps until goal / termination\")\n",
        "plt.title(\"Sarsa(0) + tile coding: learning curve (Mountain Car)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xmmYEGUxSTYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 4: Visualizing the learned Sarsa policy (video) ===\n",
        "!pip install imageio --quiet\n",
        "\n",
        "import imageio\n",
        "from IPython.display import Video\n",
        "import gymnasium as gym\n",
        "\n",
        "env_video = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "def greedy_action(state, w):\n",
        "    qs = [q_hat(state, a, w) for a in range(n_actions)]\n",
        "    return int(np.argmax(qs))\n",
        "\n",
        "def record_sarsa_policy(env, w, max_steps=200, filename=\"sarsa_mountaincar.mp4\"):\n",
        "    frames = []\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        action = greedy_action(state, w)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "    return filename\n",
        "\n",
        "video_path = record_sarsa_policy(env_video, w_sarsa)\n",
        "Video(video_path, embed=True)\n"
      ],
      "metadata": {
        "id": "pMJoM9qyG8wD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}